{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Analysis metrics DenseNet 3D\n",
    "df_fold_0 = pd.read_csv('/home/ubuntu/tenerife/data/LungAmbition/Images/ResultsBaselines_BMY0/3D/ResNet_crop32/Results/metrics_df_test_fold0_ResNetPretrained_crop32_smallDataAug_27march25.csv')\n",
    "df_fold_12 = pd.read_csv('/home/ubuntu/tenerife/data/LungAmbition/Images/ResultsBaselines_BMY0/3D/ResNet_crop32/Results/metrics_df_test_fold2_ResNetPretrained_crop32_smallDataAug_24march25.csv')\n",
    "# merge df_fold_0 and df_fold_12\n",
    "fold_metrics_df = pd.concat([df_fold_0, df_fold_12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC: 0.655129894260329 Std AUC: 0.05234476554944957\n",
      "Mean Accuracy: 0.6012369105348233 Std Accuracy: 0.08062554509003546\n",
      "Mean Balanced accuracy: 0.6070994223168137 Std Balanced accuracy: 0.09276390870741842\n",
      "Mean Precision: 0.28442028985507245 Std Precision: 0.2516681897991357\n",
      "Mean Recall/Sensitivity: 0.6153846153846154 Std Recall: 0.5384615384615385\n",
      "Mean F1-score: 0.38552188552188554 Std F1-score: 0.3354818017653358\n",
      "Mean Specificity: 0.5988142292490118 Std Specificity: 0.3565372949114167\n",
      "Mean NPV: 0.8408748114630468 Std NPV: 0.16182929655860875\n"
     ]
    }
   ],
   "source": [
    "mean_auc = fold_metrics_df['AUC'].mean()\n",
    "std_auc = fold_metrics_df['AUC'].std()\n",
    "mean_accuracy = fold_metrics_df['Accuracy'].mean()\n",
    "std_accuracy = fold_metrics_df['Accuracy'].std()\n",
    "mean_balanced_accuracy = fold_metrics_df['Bal_accuracy'].mean()\n",
    "std_balanced_accuracy = fold_metrics_df['Bal_accuracy'].std()\n",
    "mean_precision = fold_metrics_df['Precision'].mean()\n",
    "std_precision = fold_metrics_df['Precision'].std()\n",
    "mean_recall = fold_metrics_df['Sensitivity'].mean()\n",
    "std_recall = fold_metrics_df['Sensitivity'].std()\n",
    "mean_f1 = fold_metrics_df['F1-score'].mean()\n",
    "std_f1 = fold_metrics_df['F1-score'].std()\n",
    "mean_specificity = fold_metrics_df['Specificity'].mean()\n",
    "std_specificity = fold_metrics_df['Specificity'].std()\n",
    "mean_NPV = fold_metrics_df['NPV'].mean()\n",
    "std_NPV = fold_metrics_df['NPV'].std()\n",
    "# print metrics\n",
    "print(\"Mean AUC:\", mean_auc, \"Std AUC:\", std_auc)\n",
    "print(\"Mean Accuracy:\", mean_accuracy, \"Std Accuracy:\", std_accuracy)\n",
    "print(\"Mean Balanced accuracy:\", mean_balanced_accuracy, \"Std Balanced accuracy:\", std_balanced_accuracy)\n",
    "print(\"Mean Precision:\", mean_precision, \"Std Precision:\", std_precision)\n",
    "print(\"Mean Recall/Sensitivity:\", mean_recall, \"Std Recall:\", std_recall)\n",
    "print(\"Mean F1-score:\", mean_f1, \"Std F1-score:\", std_f1)\n",
    "print(\"Mean Specificity:\", mean_specificity, \"Std Specificity:\", std_specificity)\n",
    "print(\"Mean NPV:\", mean_NPV, \"Std NPV:\", std_NPV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lungAmbpy3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
